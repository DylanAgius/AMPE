%===================================================================================
\section{Fortran example problems}\label{s:ex_fortran}
%===================================================================================

The {\F} example problem programs supplied with the {\cvode}
package are all written in standard {\F}77 and use double-precision
arithmetic. However, when the {\F} examples are built, the source code is
automatically modified according to the configure options supplied by the
user and the system type. Integer variables are declared as {\tt INTEGER*}{\em n},
where {\em n} denotes the number of bytes in the corresponding {\C} type
({\tt long int} or {\tt int}). Floating-point variable declarations remain
unchanged if double-precision is used, but are changed to {\tt REAL*}{\em n},
where {\em n} denotes the number of bytes in the {\sundials} type {\tt realtype},
if using single-precision. Also, if using single-precision, then declarations of
floating-point constants are appropriately modified; e.g. {\tt 0.5D-4} is
changed to {\tt 0.5E-4}.


\subsection{A serial example: fcvkryx}\label{ss:fcvkryx}

The \id{fcvkryx} example is a {\F} equivalent of the \id{cvkryx} problem.
(In fact, it was derived from an earlier {\F} example program for VODPK.)
The source program \id{fcvkryx.f} is listed in Appendix \ref{s:fcvkryx_f}.

The main program begins with a call to \id{INITKX}, which sets problem
parameters, loads these into arrays \id{IPAR} and \id{RPAR} for use by other
routines, and loads \id{Y} with its initial values.  It calls \id{FNVINITS},
\id{FCVMALLOC}, \id{FCVSPGMR}, \id{FCVSPGMRSETPSET}, and \id{FCVSPGMRSETPSOL} to
initialize the {\nvecs} module, the main solver memory, and the {\cvspgmr} module,
and to specify user-supplied preconditioner setup and solve routines.
It calls \id{FCVODE} in a loop over \id{TOUT} values, with printing of
selected solution values and performance data (from the \id{IOUT}  and \id{ROUT}
arrays).  At the end, it prints a number of performance counters, and
frees memory with calls to \id{FCVFREE}.

In \id{fcvkryx.f}, the \id{FCVFUN} routine is a straghtforward implementation
of the discretized form of Eqns. (\ref{cvkryxpde}).  In \id{FCVPSET}, the
block-diagonal part of the Jacobian, $J_{bd}$, is computed (and copied to
\id{P}) if \id{JOK} = 0, but is simply copied from \id{BD} to \id{P} if
\id{JOK} = 1.  In both cases, the preconditioner matrix $P$ is formed from 
$J_{bd}$ and its $2 \times 2$ blocks are LU-factored.  In \id{FCVPSOL},
the solution of a linear system $Px = z$ is solved by doing backsolve
operations on the blocks.  The remainder of \id{fcvkryx.f} consists of
routines from LINPACK and the BLAS needed for matrix and vector operations.

The following is sample output from \id{fcvkryx}, using a $10 \times 10$ mesh.
The performance of {\fcvode} here is quite similar to that of {\cvode} on
the \id{cvkryx} problem, as expected.

%%
\includeOutput{fcvkryx}{../../examples/cvode/fcmix_serial/fcvkryx.out}
%%

%-----------------------------------------------------------------------------------

\subsection{A parallel example: fcvkryx\_bbd\_p}\label{ss:fcvkryx_bbd_p}

This example, \id{fcvkryx\_bbd\_p}, uses a simple diagonal ODE system to illustrate
the use of {\fcvode} in a parallel setting.  The system is
\begin{equation} \label{diagode}
\dot{y}_i = - \alpha ~i~ y_i ~~~ (i = 1,\ldots, N)
\end{equation}
on the time interval $0 \leq t \leq 1$.  In this case, we use $\alpha = 10$
and $N = 10*$\id{NPES}, where \id{NPES} is the number of processors
and is specified at run time.  The linear solver to be used is
{\spgmr} with the {\cvbbdpre} (band-block-diagonal) preconditioner.
Since the system Jacobian is diagonal, the half-bandwidths specified
are all zero.  The problem is solved twice --- with preconditioning on
the left, then on the right.

The source file, \id{fcvkryx\_bbd\_p.f}, is listed in Appendix \ref{s:fcvkryx_bbd_p_f}.
It begins with MPI calls to initialize MPI and to get the number of processors
and local processor index.  The linear solver specification is done with
calls to \id{FCVBBDINIT} and \id{FCVBBDSPGMR}.  In a loop over \id{TOUT}
values, it calls \id{FCVODE} and prints the step and $f$ evaluation counters.
After that, it computes and prints the maximum global error, and all the
relevant performance counters.  Those specific to {\cvbbdpre} are obtained
by a call to \id{FCVBBDOPT}.  To prepare for the second run, the program
calls \id{FCVREINIT}, \id{FCVBBDREINIT}, and \id{FCVSPGMRREINIT}, in addition
to resetting the initial conditions.  Finally, it frees memory and terminates MPI.
Notice that in the \id{FCVFUN} routine, the local processor index \id{MYPE}
and the local vector size \id{NLOCAL} are used to form the global index
values needed to evaluate the right-hand side of Eq. (\ref{diagode}).

The following is a sample output from \id{fcvkryx\_bbd\_p}, with \id{NPES} = 4.
As expected, the performance is identical for left vs right preconditioning.

%%
\includeOutput{fcvkryx\_bbd\_p}{../../examples/cvode/fcmix_parallel/fcvkryx_bbd_p.out}
%%

%-----------------------------------------------------------------------------------
